# Deep Neural Networks with PyTorch
<!-- GFM-TOC -->
* [Deep Neural Networks with PyTorch](#Deep-Neural-Networks-with-PyTorch)
    * [1. Syllabus](#1-Syllabus)
    * [2. Improving Deep Neural Networks](#2-Improving-Deep-Neural-Networks)
<!-- GFM-TOC -->
This is an online course offered by Coursera. This course introduces how to develop deep learning models using Pytorch. 
Starting with the Pytorch's tensors, each section covers different models such as Linear Regression, and logistic/softmax regression.
Followed by Feedforward deep neural networks, the role of different activation functions, 
normalization and dropout layers. Then Convolutional Neural Networks and Transfer learning are also covered.

##  1. Syllabus
### Chapter 1 Tensor and Datasets
[1\. Tensor](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter01_01Tensor.py
)\
[2\. Dataset](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter01_02Dataset.py)\
[3\. Complex Dataset](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter01_03Complex_Dataset.py)

### Chapter 2 Linear Regression
[1\. Linear Regression with One Parameter](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter02_01LinearRegression_1P.py)\
[2\. Linear Regression - Prediction](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter02_02LinearRegression.py)\
[3\. Linear Regression with Two Parameters](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter02_03LinearRegression_2P.py)

### Chapter 3 Linear Regression PyTorch Way
[1\. Stochastic Gradient Descent](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter03_01StochasticGradientDescent.py)\
[2\. Linear Regression with SGD](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter03_02LR_SGD.py)\
[3\. Training and Validation Data with Pytorch](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter03_03Validation.py)

### Chapter 4 Multiple Input Output Linear Regression
[1\. Multiple Input Linear Regression ](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter04_01MultipleLR.py)\
[2\. Multiple Output Linear Regression](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter04_02MultipleOutputLR.py)\

### Chapter 5 Logistic Regression for Classification
[1\. Linear Classifier](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter05_01LinearClassifier.py)\
[2\. Logistic Regression](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter05_02LogisticRegression.py)

### Chapter 6 Softmax Rergresstion
[1\. Softmax in 1D](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter06_01Softmax1D.py)\
[2\. Softmax Classifier](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter06_02SoftmaxClassifier.py)

### Chapter 7 Shallow Neural Networks
[1\. Neural Network in 1D](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter07_01NN1D.py)\
[2\. Neural Network in 1D with Multiple Nodes](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter07_02NN1D_MultiNode.py)\
[3\. Neural Network with Multiple Dimensional Input](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter07_03NNMultiDim.py)\
[4\. Neural Network with Multiple Classes](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter07_04NNMultiClass.py)\
[5\. Activation Functions](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter07_05ActivationFunctions.py)

### Chapter 8 Deep Networks
[1\. Multiple Hidden Layer Deep Network](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_01MultiLayer.py)\
[2\. Multiple Hidden layer with ModuleList](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_02MultiLayer_ModuleList.py)\
[3\. Using Dropout for Classification](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_03Dropout.py)\
[4\. Using Dropout for Regression](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_04DropoutRegression.py)\
[5\. Initialization Weights](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_05InitializationWeights.py)\
[6\. Different Initialization Methods](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_06DifferentInitialization.py)\
[7\. Momentum](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_07Momentum.py)\
[8\. Neural Network with Momentum](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_08NNwithMomentum.py)\
[9\. Batch Normalization](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_09BatchNormalization.py)\
[10\. Batch Normalization and Dropout](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_10Batch%26Dropout.py)

### Chapter 9 Convolutional Neural Network
[1\. Convolution](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_01Convolution.py)\
[2\. Activation Function and Max Pooling](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_02ActFunc%26MaxPool.py)\
[3\. Multiple IO Channels](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_03MultiIN%26MultiOUT.py)\
[4\. Convolutional Neural Network](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_04CNN.py)\
[5\. CNN with MNIST](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_05CNNwithMNIST.py)\
[6\. CNN with Batch Normalization](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_06CNNwithBatchNormalization.py)


##  2. Improving Deep Neural Networks
### 2.1 Practical aspects of Deep Learning
####  2.1.1 Setting up your ML Application
1. Seperate total data into training set, development set(for cross validation) and test set. Normally, they are seperated by 3/1/1 or, 7/3/0. When we have a huge size of data, we normally reduce the size of dev set and test set significantly.
2. Make sure the data in dev set and test set come from the same distribution. For an example, the pictures are in the training set are from experters, and the test pictures are from users using app and blurry.
3. Bias and variance. Underfitting => high bias, overfitting => high variance.\
When the error in the dev set is higher than that is in the training set, then the model is most likely overfitted with high variance. When the error in both dev and training set is high, then model is most likely underfitted with high bias. The model could also be with high bias (large error in training set )and high variance (large error in dev set).
4. Basic recipe for ML\
   4.1. High bias? (training set performance)  -->  Try bigger network/more layers/more neurons (may reduce bias without hurting variance), train longer or maybe other NN architecture\ 
   4.2. High variance? (dev set performance)  -->  More data (may reduce variance without hurting bias), regularization or maybe other NN architecture\
   4.3. Training a bigger network never hurts. The only drawback is the computational load.

####  2.1.2 Regularizing your neural network

























# Deep Neural Networks with PyTorch
<!-- GFM-TOC -->
* [1. Syllabus](#1-Syllabus)
    * [Chapter 1 Tensor and Datasets](#Chapter-1-Tensor-and-Datasets)
    * [Chapter 2 Linear Regression](#Chapter-2-Linear-Regression)
    * [Chapter 3 Linear Regression PyTorch Way](#Chapter-3-Linear-Regression-PyTorch-Way)
    * [Chapter 4 Multiple Input Output Linear Regression](#Chapter-4-Multiple-Input-Output-Linear-Regression)
    * [Chapter 5 Logistic Regression for Classification](#Chapter-5-Logistic-Regression-for-Classification)
    * [Chapter 6 Softmax Rergresstion](#Chapter-6-Softmax-Rergresstion)
    * [Chapter 7 Shallow Neural Networks](Chapter-7-Shallow-Neural-Networks)
    * [Chapter 8 Deep Networks](#Chapter-8-Deep-Networks)
    * [Chapter 9 Convolutional Neural Network](#Chapter-9-Convolutional-Neural-Network)
<!-- GFM-TOC -->
<!-- GFM-TOC -->
* [2. Improving Deep Neural Networks](#2-Improving-Deep-Neural-Networks)
    * [2.1 Practical Aspects of Deep Learning](#21-Practical-Aspects-of-Deep-Learning)
    * [2.2 Optimization Algorithms](#22-Optimization-Algorithms)
<!-- GFM-TOC -->
This is an online course offered by Coursera. This course introduces how to develop deep learning models using Pytorch. 
Starting with the Pytorch's tensors, each section covers different models such as Linear Regression, and logistic/softmax regression.
Followed by Feedforward deep neural networks, the role of different activation functions, 
normalization and dropout layers. Then Convolutional Neural Networks and Transfer learning are also covered.

##  1. Syllabus
### Chapter 1 Tensor and Datasets
[1\. Tensor](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter01_01Tensor.py
)\
[2\. Dataset](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter01_02Dataset.py)\
[3\. Complex Dataset](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter01_03Complex_Dataset.py)

### Chapter 2 Linear Regression
[1\. Linear Regression with One Parameter](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter02_01LinearRegression_1P.py)\
[2\. Linear Regression - Prediction](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter02_02LinearRegression.py)\
[3\. Linear Regression with Two Parameters](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter02_03LinearRegression_2P.py)

### Chapter 3 Linear Regression PyTorch Way
[1\. Stochastic Gradient Descent](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter03_01StochasticGradientDescent.py)\
[2\. Linear Regression with SGD](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter03_02LR_SGD.py)\
[3\. Training and Validation Data with Pytorch](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter03_03Validation.py)

### Chapter 4 Multiple Input Output Linear Regression
[1\. Multiple Input Linear Regression ](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter04_01MultipleLR.py)\
[2\. Multiple Output Linear Regression](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter04_02MultipleOutputLR.py)\

### Chapter 5 Logistic Regression for Classification
[1\. Linear Classifier](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter05_01LinearClassifier.py)\
[2\. Logistic Regression](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter05_02LogisticRegression.py)

### Chapter 6 Softmax Rergresstion
[1\. Softmax in 1D](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter06_01Softmax1D.py)\
[2\. Softmax Classifier](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter06_02SoftmaxClassifier.py)

### Chapter 7 Shallow Neural Networks
[1\. Neural Network in 1D](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter07_01NN1D.py)\
[2\. Neural Network in 1D with Multiple Nodes](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter07_02NN1D_MultiNode.py)\
[3\. Neural Network with Multiple Dimensional Input](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter07_03NNMultiDim.py)\
[4\. Neural Network with Multiple Classes](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter07_04NNMultiClass.py)\
[5\. Activation Functions](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter07_05ActivationFunctions.py)

### Chapter 8 Deep Networks
[1\. Multiple Hidden Layer Deep Network](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_01MultiLayer.py)\
[2\. Multiple Hidden layer with ModuleList](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_02MultiLayer_ModuleList.py)\
[3\. Using Dropout for Classification](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_03Dropout.py)\
[4\. Using Dropout for Regression](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_04DropoutRegression.py)\
[5\. Initialization Weights](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_05InitializationWeights.py)\
[6\. Different Initialization Methods](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_06DifferentInitialization.py)\
[7\. Momentum](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_07Momentum.py)\
[8\. Neural Network with Momentum](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_08NNwithMomentum.py)\
[9\. Batch Normalization](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_09BatchNormalization.py)\
[10\. Batch Normalization and Dropout](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter08_10Batch%26Dropout.py)

### Chapter 9 Convolutional Neural Network
[1\. Convolution](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_01Convolution.py)\
[2\. Activation Function and Max Pooling](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_02ActFunc%26MaxPool.py)\
[3\. Multiple IO Channels](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_03MultiIN%26MultiOUT.py)\
[4\. Convolutional Neural Network](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_04CNN.py)\
[5\. CNN with MNIST](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_05CNNwithMNIST.py)\
[6\. CNN with Batch Normalization](https://github.com/yshiyi/Deep-Neural-Networks-with-PyTorch/blob/main/Chapter09_06CNNwithBatchNormalization.py)


##  2. Improving Deep Neural Networks
### 2.1 Practical Aspects of Deep Learning
####  2.1.1 Setting up your ML Application
1. Seperate total data into training set, development set(for cross validation) and test set. Normally, they are seperated by 3/1/1 or, 7/3/0. When we have a huge size of data, we normally reduce the size of dev set and test set significantly.
2. Make sure the data in dev set and test set come from the same distribution. For an example, the pictures are in the training set are from experters, and the test pictures are from users using app and blurry.
3. Bias and variance. Underfitting => high bias, overfitting => high variance.\
When the error in the dev set is higher than that is in the training set, then the model is most likely overfitted with high variance. When the error in both dev and training set is high, then model is most likely underfitted with high bias. The model could also be with high bias (large error in training set )and high variance (large error in dev set).
4. Basic recipe for ML\
   4.1. High bias? (training set performance)  -->  Try bigger network/more layers/more neurons (may reduce bias without hurting variance), train longer or maybe other NN architecture\
   4.2. High variance? (dev set performance)  -->  More data (may reduce variance without hurting bias), regularization or maybe other NN architecture\
   4.3. Training a bigger network never hurts. The only drawback is the computational load.

####  2.1.2 Regularizing your neural network
1. L2 regularization is also named as weight decay. w_new = (1 - regularization term) * w_old - lr * dw.\
   It works just lik the ordinally gradient descent, where you update w by subtracting lr times the original gradient you got from backprop. But now you are also multiplying w by this thing, which is a little less than 1.
2. How does regularization prevent overfitting?\
   The extra regularization term penalizes the weight matrics from being too large.\
   Increasing the regularization term, we can reduce the value of the corresponding weight. When the regularization term is large, the weight is close to zero. Then that node will be zeroed out.
3. Dropout regularization (Inverted dropout)\
   3.1. Dropout is one of regularization methods to prevent overfitting.
   3.2. activation = activation / dropout prob. Make sure the expectation remains the same.\
4. Other regularization\
   4.1. Data augmentation, transform photos (flip, rotate or distortion)
   4.2. Early stopping. Stop training before dev set error gets larger.
   
####  2.1.3 Setting up your optimization problem
1. Normalizaing inputs will speed up training. Without normalizing inputs, we have to use small learning rate. If the features come from very different scales, then it's important to normalize features to help learning algorithm run faster.
2. Initialize weights properly can prevent vanishing or exploding gradients in a very deep network.


### 2.2 Optimization Algorithms
####  2.2.1 Mini-Batch Gradient Descent
1. One epoch denotes a single pass through training set.
2. Choosing mini-batch size:\
   2.1. If mini-batch size = training set size (m): batch gradient descent (low noise, relatively large step, take too long per iteration)\
   2.2. If mini-batch size is 1: stochastic gradietn descent (single step, can be extremely noisy, won't converge to the global minimum, just wonder round the region of minimum)\
   2.3. In pratice, mini-batch size is between 1 and m: fastest learning, make progress without processing entire training set\
   2.4. Small training set (<= 2000)- batch gradient descent\
   2.5. Mini-batch size is another hyperparameter, try different values and find out the one that makes the gradient descent optimization algorithm as efficient as possible.

####  2.2.2 Exponentially Weighted Averages
```
V_t = \beta * V_t-1 + (1 - \beta) * \theta_t
beta: weighting factor
theta_t: true value in the current time/step
V_0: assigned the initial weighted value
V_t: weighted value in the current time/step
V_t-1: weighted value in the previous time/step
```
We can think V_t as approximately averaging over 1/(1-\beta). For example, if \beta = 0.9, we can think of this as averaging over the last 10 true values.\
For example:\
```
\beta = 0.9
V_100 = 0.1*\theta_100 + 0.1*0.9*\theta_99 + 0.1*0.9^2*\theta_98 + ... + 0.1*0.9^n*\theta_(100-n) + ... + 0.1*0.9^99*\theta_1
The exponentially decaying function reduces from 0.1 to 0.1*0.9^99.
The way to compute V_100 is to take the element wise product between this exponentially decaying function and the true values and sum it up.
Note: 0.9^10 ~= 0.35 = 1/e, This means it takes about 10 steps for the true value to decay to around 1/3 of the peak. Therefore, when \beta = 0.9, this is as if we are computing an exponentially weighted average that focused on just the last 10 true values.
```














